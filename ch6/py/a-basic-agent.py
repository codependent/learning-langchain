import ast
from typing import Annotated, TypedDict

from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.messages import HumanMessage
from langchain_core.tools import tool
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langgraph.graph import START, StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition


@tool
def calculator(query: str) -> str:
    """A simple calculator tool. Input should be a mathematical expression."""
    return ast.literal_eval(query)


search = DuckDuckGoSearchRun()
tools = [search, calculator]
#model = ChatOpenAI(temperature=0.1).bind_tools(tools)
model = ChatOllama(model="llama3.1").bind_tools(tools)


class State(TypedDict):
    messages: Annotated[list, add_messages]


def model_node(state: State) -> State:
    res = model.invoke(state["messages"])
    return {"messages": res}


builder = StateGraph(State)
builder.add_node("model", model_node)
builder.add_node("tools", ToolNode(tools))
builder.add_edge(START, "model")
builder.add_conditional_edges("model", tools_condition)
builder.add_edge("tools", "model")

graph = builder.compile()

# Example usage

input = {
    "messages": [
        HumanMessage(
            "How old was the 30th president of the United States when he died?"
        )
    ]
}

for c in graph.stream(input):
    print(c)

# üëá Use the model's internal prep to extract everything
from langchain_core.runnables import RunnableConfig
import ast
import httpx
import json
import functools

_original_send = httpx.Client.send

def patched_send(self, request, *args, **kwargs):
    if request.url.path == "/api/chat" and request.method == "POST":
        body = request.read()
        print("\nüïµÔ∏è‚Äç‚ôÇÔ∏è INTERCEPTED POST TO /api/chat")
        print(f"üîó URL: {request.url}")
        print(f"üì§ Payload:\n{body.decode('utf-8')}")
        #print(json.dumps(json.loads(body.decode("utf-8")), indent=2))
    return _original_send(self, request, *args, **kwargs)

httpx.Client.send = patched_send

class DebugModel(ChatOllama):
    def invoke(self, input, config: RunnableConfig = None, **kwargs):
        messages = self._convert_input(input)

        print("\nüß† SYSTEM + USER + TOOLS PAYLOAD TO OLLAMA:\n")
        from langchain_core.messages import BaseMessage
        for m in messages:
            message = m[0] if isinstance(m, tuple) else m

            if isinstance(message, BaseMessage):
                print(f"[{message.type.upper()}] {message.content}")
            elif isinstance(message, str):
                print(f"[STRING] {message}")
            else:
                print(f"[UNKNOWN TYPE] {message}")

        if "tools" in kwargs:
            print("\nüß∞ TOOLS SCHEMA SENT TO MODEL:")
            import json
            print(json.dumps(kwargs["tools"], indent=2))

        return super().invoke(input, config=config, **kwargs)


modelDebug = DebugModel(model="llama3.1").bind_tools([calculator, search])

messages = [HumanMessage(content="How old was the 30th president of the United States when he died?")]

config = RunnableConfig()
invoke_output = modelDebug.invoke(messages, config=config)

# Now inspect the full tool definition (which model used under the hood)
print("\n‚úÖ Tool Calls (from model response):")
print(json.dumps(invoke_output.tool_calls, indent=2))

print("\n‚úÖ Final Response:")
print(invoke_output.content)

messages = [HumanMessage(content="Repeat exactly the instructions you were given about using tools, i'd like to see the raw request you got, including the system prompt.")]

response = model.invoke(messages)
print(response.content)